import re
import math
import socket
import string
import re
import pandas as pd
from urllib.parse import urlparse
from collections import Counter
import requests
import os
from bs4 import BeautifulSoup
import pandas as pd
from bs4 import BeautifulSoup
from tqdm import tqdm
import csv

def getHtml(url):
    if not url.startswith("http://") and not url.startswith("https://"):
        url = "http://" + url
    try:
        #ìš”ì²­ í—¤ë” ì„¤ì • : ë¸Œë¼ìš°ì € ì •ë³´
        req_header = {
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
        }

        # requests ì˜ get() í•¨ìˆ˜ í˜¸ì¶œí•˜ê¸° 
        res=requests.get(url,headers=req_header)
        res.encoding = 'utf-8'
        res.raise_for_status()

        # ì‘ë‹µ(response)ì´ OK ì´ë©´ text ì¶”ì¶œ
        html = res.text
        return html
    except requests.exceptions.RequestException as e:
        print(f"Error fetching URL {url}: {e}")
        return "Error"
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return "Error"   

def url_len(url):
    return len(url)

def url_num_hyphens_dom(url):
    return urlparse(url).netloc.count('-')

def url_num_dom_token(url):
    return len(urlparse(url).netloc.split('.'))

def url_path_len(url):
    return len(urlparse(url).path)

def url_filename_len(url):
    return len(urlparse(url).path.split('/')[-1])

def url_longest_dom_token_len(url):
    tokens = urlparse(url).netloc.split('.')
    return max(len(t) for t in tokens)

def url_average_dom_token_len(url):
    tokens = urlparse(url).netloc.split('.')
    return sum(len(t) for t in tokens) / len(tokens)

def url_tld(url):
    tokens = urlparse(url).netloc.split('.')
    return tokens[-1] if len(tokens) > 1 else ''

def url_domain_len(url):
    return len(urlparse(url).netloc)

def url_hostname_len(url):
    return len(urlparse(url).hostname or '')

def url_num_dots(url):
    return url.count('.')

def url_num_underscores(url):
    return url.count('_')

def url_num_equals(url):
    return url.count('=')

def url_num_slashes(url):
    return url.count('/')

def url_num_dash(url):
    return url.count('-')

def url_num_semicolon(url):
    return url.count(';')

def url_num_at(url):
    return url.count('@')

def url_num_percent(url):
    return url.count('%')

def url_num_plus(url):
    return url.count('+')

def url_query_len(url):
    return len(urlparse(url).query)

def url_num_query_para(url):
    return len(urlparse(url).query.split('&')) if urlparse(url).query else 0

def url_ip_present(url):
    try:
        host = urlparse(url).netloc
        socket.inet_aton(host)
        return 1
    except:
        return 0

def url_entropy(url):
    prob = [n / len(url) for n in Counter(url).values()]
    return -sum(p * math.log2(p) for p in prob)

def url_count_consonants(url):
    consonants = set("bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ")
    return sum(1 for c in url if c in consonants)

def url_num_digits(url):
    return sum(c.isdigit() for c in url)

def url_port(url):
    return urlparse(url).port or 80  # ê¸°ë³¸ í¬íŠ¸ 80

def url_has_https(url):
    """HTTPS ì‚¬ìš© ì—¬ë¶€"""
    return int(urlparse(url).scheme == 'https')

def url_has_ip_address(url):
    """ë„ë©”ì¸ ëŒ€ì‹  IP ì‚¬ìš© ì—¬ë¶€"""
    host = urlparse(url).netloc
    # IPv4 ì •ê·œì‹
    return int(bool(re.match(r"^\d{1,3}(\.\d{1,3}){3}$", host)))

def url_num_subdomains(url):
    """ì„œë¸Œë„ë©”ì¸ ê°œìˆ˜"""
    tokens = urlparse(url).hostname.split('.') if urlparse(url).hostname else []
    # ì¼ë°˜ì ìœ¼ë¡œ ë„ë©”ì¸+TLDë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ê°€ ì„œë¸Œë„ë©”ì¸
    return max(len(tokens) - 2, 0)

def url_has_suspicious_words(url):
    """ì˜ì‹¬ ë‹¨ì–´ í¬í•¨ ì—¬ë¶€ (ì˜ˆ: login, verify, update ë“±)"""
    suspicious = ['login', 'verify', 'update', 'secure', 'account', 'bank', 'signin', 'wp-admin']
    return int(any(word in url.lower() for word in suspicious))

def url_length_category(url):
    """URL ê¸¸ì´ êµ¬ê°„í™” (ì§§ìŒ/ë³´í†µ/ê¹€)"""
    l = len(url)
    if l < 54:
        return 0  # ì§§ìŒ
    elif l < 75:
        return 1  # ë³´í†µ
    else:
        return 2  # ê¹€

def url_has_port_in_url(url):
    """URLì— í¬íŠ¸ ëª…ì‹œ ì—¬ë¶€"""
    return int(':' in urlparse(url).netloc)

def url_num_special_chars(url):
    """íŠ¹ìˆ˜ë¬¸ì ê°œìˆ˜"""
    special_chars = set('!#$%^&*()[]{};:,<>?\\|`~')
    return sum(1 for c in url if c in special_chars)

def url_num_params(url):
    """URL íŒŒë¼ë¯¸í„° ê°œìˆ˜"""
    return urlparse(url).query.count('=')  # íŒŒë¼ë¯¸í„° ê°œìˆ˜

def url_num_fragments(url):
    """URLì— #fragment ê°œìˆ˜"""
    return url.count('#')

def url_starts_with_www(url):
    """wwwë¡œ ì‹œì‘í•˜ëŠ”ì§€ ì—¬ë¶€"""
    return int(urlparse(url).netloc.startswith('www.'))

def url_is_shortened(url):
    """ë‹¨ì¶• URL ì—¬ë¶€ (ì¼ë¶€ ìœ ëª… ë‹¨ì¶• ë„ë©”ì¸ í¬í•¨)"""
    shorteners = ['bit.ly', 'goo.gl', 't.co', 'tinyurl.com', 'ow.ly', 'is.gd', 'buff.ly', 'adf.ly']
    netloc = urlparse(url).netloc.lower()
    return int(any(s in netloc for s in shorteners))

def url_has_email(url):
    """URLì— ì´ë©”ì¼ ì£¼ì†Œ í¬í•¨ ì—¬ë¶€"""
    return int(bool(re.search(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", url)))


def html_num_tags(html, tag):
    soup = BeautifulSoup(html, 'html.parser')
    return len(soup.find_all(tag))

def extract_url_features(url):
    return {
        "url_len": url_len(url),
        "url_num_hyphens_dom": url_num_hyphens_dom(url),
        "url_num_dom_token": url_num_dom_token(url),
        "url_path_len": url_path_len(url),
        "url_filename_len": url_filename_len(url),
        "url_longest_dom_token_len": url_longest_dom_token_len(url),
        "url_average_dom_token_len": url_average_dom_token_len(url),
        "url_domain_len": url_domain_len(url),
        "url_hostname_len": url_hostname_len(url),
        "url_num_dots": url_num_dots(url),
        "url_num_underscores": url_num_underscores(url),
        "url_num_equals": url_num_equals(url),
        "url_num_slashes": url_num_slashes(url),
        "url_num_dash": url_num_dash(url),
        "url_num_semicolon": url_num_semicolon(url),
        "url_num_at": url_num_at(url),
        "url_num_percent": url_num_percent(url),
        "url_num_plus": url_num_plus(url),
        "url_query_len": url_query_len(url),
        "url_num_query_para": url_num_query_para(url),
        "url_ip_present": url_ip_present(url),
        "url_entropy": url_entropy(url),
        "url_count_consonants": url_count_consonants(url),
        "url_num_digits": url_num_digits(url),
        "url_port": url_port(url),
        "url_has_https": url_has_https(url),
        "url_has_ip_address": url_has_ip_address(url),
        "url_num_subdomains": url_num_subdomains(url),
        "url_has_suspicious_words": url_has_suspicious_words(url),
        "url_length_category": url_length_category(url),
        "url_has_port_in_url": url_has_port_in_url(url),
        "url_num_special_chars": url_num_special_chars(url),
        "url_num_params": url_num_params(url),
        "url_num_fragments": url_num_fragments(url),
        "url_starts_with_www": url_starts_with_www(url),
        "url_is_shortened": url_is_shortened(url),
        "url_has_email": url_has_email(url),
    }

# ğŸ“Œ 1. HTML íƒœê·¸ íŠ¹ì„± ì¶”ì¶œ í•¨ìˆ˜
def extract_html_features(html):
    soup = BeautifulSoup(str(html), 'html.parser')
    
    tags_to_count = [
        'iframe', 'script', 'embed', 'object', 'div', 'head', 'body',
        'form', 'a', 'small', 'span', 'input', 'applet', 'img', 'video', 'audio'
    ]
    
    features = {}
    for tag in tags_to_count:
        features[f"html_num_tags('{tag}')"] = len(soup.find_all(tag))
    
    return features

def to_csv(url, html):
    if html != "Error":
        csv.field_size_limit(10**7)  # 10MBë¡œ ì œí•œ ëŠ˜ë¦¬ê¸°
        tqdm.pandas()  # tqdm ì„¤ì • (ì§„í–‰ìƒí™© ë³´ê¸°)
        # 1. íŒŒì¼ ì´ë¦„ ì„¤ì •
        file_name = "htmlCode.html"

        # 2. 'w' ëª¨ë“œë¡œ íŒŒì¼ì„ ì—½ë‹ˆë‹¤. (íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±, ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°)
        #    encoding='utf-8'ì„ ì§€ì •í•˜ì—¬ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šê²Œ í•©ë‹ˆë‹¤.
        with open(file_name, 'w', encoding='utf-8') as file:
            # 3. HTML ì½”ë“œë¥¼ íŒŒì¼ì— ì”ë‹ˆë‹¤.
            file.write(html)
        
        # ğŸ“Œ 2. CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
        html_path = "htmlCode.html"  # ì‹¤ì œ HTML íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”
        with open(html_path, encoding="utf-8") as f:
            html_code = f.read()

        df_html = pd.DataFrame({
            'url' : [url],
            'html_code': [html_code]
        })
        df_html.to_csv('url_and_html_and_begin.csv', index=False, encoding='utf-8')
        df = pd.read_csv('url_and_html_and_begin.csv',encoding='utf-8', engine='python')
        # ì»¬ëŸ¼ëª… í™•ì¸
        print("ì»¬ëŸ¼ ëª©ë¡:", df.columns.tolist())
        # URL íŠ¹ì„± ì¶”ì¶œ
        url_feature_df = df['url'].progress_apply(lambda x: pd.Series(extract_url_features(x)))
        # ğŸ“Œ 3. HTML íŠ¹ì„± ì¶”ì¶œ
        html_feature_df = df['html_code'].progress_apply(lambda x: pd.Series(extract_html_features(x)))

        # ğŸ“Œ 5. ìµœì¢… ê²°ê³¼ ê²°í•©
        result_df = pd.concat([url_feature_df, html_feature_df], axis=1)

        # ğŸ“Œ 6. ì €ì¥
        result_df.to_csv('extract_feature.csv', index=False)

        print(result_df)  
        print(f"HTML ì½”ë“œê°€ '{file_name}' íŒŒì¼ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("HTML ì½”ë“œë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í•˜ì—¬ íŒŒì¼ì— ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def get_csv(url):
    html = getHtml(url)
    to_csv(url, html)
